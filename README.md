# NOTE:

This is a code sample I prepared for an interview that had a particular syntax and format wanted. I put a decent amount of time into making it and collecting up a lot of my work so I figured I may as well just post it, as it has more updated work than most things. The simplest way to view this would be to look at visualizations/visualization.html as it has all other code inside it if you expand the code chunks. You will likely have to download the html file locally. 

Restrictions of what the position wanted meant I did everything here in R, but I am also proficient in Python based tooling (my current public code is a bit outdated at the moment for python, though). 


# Freedman's Bureau Sample Data Processing

## Folder Structure 

README.md | This file, readme for the data processing

code-sample.R | The R code this readme describes

source_data | a directory including source data loaded into this script

visualization | folder with all the visualization information

- visualization.html | main visualization itself

- visualization.qmd | source code for visualization

- outputs | outputs generated by code-sample.R

- unrelated_figures | directory where I keep the raw visualization images for extra examples at the end of the visualization. 

## Description and what it solves.

This code was adapted (and heavy changed) from a dataset created partly by myself in an Economic History course. This data is matching records from the Freedman's Bureau in Williamsburg, VA to the same person in other census records. These Freedman's Bureau records and the full count census records have been digitized, but only partly, and mostly in image form. The class each was assigned a section of the records to manually attempt to digitize (from blurry scans of 1860s cursive...) a section of records. From there, we had to match them to the 1870-1900 decicennial census records. These are fully digitized in some aspects (name, etc) but for linking purposes we also had to record much from messy handwriting scans. 

I figured this would be a decent example because of the incredibly messy nature of the data. The data is extremely messily formatted with no clearly defined factors, spelling issues, and inconsistencies everywhere. The formatting is also far from ideal for working with it in a tidy way. This code is the first main step in transforming the data into a usable format for further study. This is also used in the visualzation example later (and the code exports to the output folder inside the visualization directory)

## Instructions

Simply run the code-sample.R file. If all the dependencies are installed it should write two .csv files to visualization/outputs after being run. 
NOTE: I also created a .csv in the source_data directory to make it easier to compare the inputs and outputs if you wish. I did not load this file as that was not the original format. 

## Libraries

### tidytable 

This package is a near clone of the dplyr syntax that translates to data.table commands under the hood. 
I am more familiar with dplyr syntax but the speed and efficiency gains of data.table means I typically use this as a relatively drop in replacement. 

### haven 

The input format is using Stata's .dta format. This package is simply used to load that into an R dataframe/tibble. 

### forcats 

This package is great for simplifying working with factors in R. I used it to recode quite a few different factor variables.

### stringr

This package is a package that makes manipulating strings and using regex a lot easier inside R. The messy encoding of this dataset meant I 
had to use this package quite extensively. Any command starting with str_ is a stringr command. 

### purrr

This package is used quite a lot here. Does a similar job to R's apply() family but I prefer the syntax and how 
compact it is. I don't explicitly call map() very often but I do a couple times. I use purrr's style for anonymous functions inside the across() function quite a lot, however.

### tidystringdist

A package for implimenting string distance algorithsms. Basically it's just stringdist with a UX I slightly prefer. 
## Audience/output file 

This output file is mostly just a cleaned up file with a much more 'tidy' format. The input file is quite messy, and it would be difficult to do any significant analysis with it. The primary difference is that most of the variables are cleaned up and refactored, and I used pivot_longer() so that each year is it's own discrete row. The main thing this allows is it makes it significantly easier to incorporate time in your analysis. You can simply group by ID number and then each group will be one person with each row being an observation. The refactoring was mainly around two thingsss: occupation and location. I used regex to split up the names and locations into separate columns for first and last and city/state. This also cleaned up their formatting significantly, and I checked the state level factoring by using the state.name dataset. 

The second output is for comparing the strings of the first and last names using string distancing algorithsms. I just transform it and run the algorithms (I run many, so it creates one column for each). This will later be used in my visualizations. 

I was planning on including more within this file itself, but I reached the line limit unfortunately. I think it shows a pretty good example of parsing a real world messy dataset, however. Audience wise, this dataset is made for people who want to run econometric analysis on the data itself and make visualizations. On the visualizations sample I primarily use this very dataset. 
